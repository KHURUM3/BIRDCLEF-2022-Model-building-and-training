{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2d2613",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cbcddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in ./miniconda3/lib/python3.9/site-packages (1.5.1)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install torchsummary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaae592",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601f6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# General purpose libraries for loading and manipulating data\n",
    "# Общие библиотеки для загрузки данных и взаимодействия с данными\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# Pytorch imports for neural networks and tensor manipulations\n",
    "# Импорт Pytorch для созданий нейросетей и взаимодействия с тензором\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.transforms import Resize\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "# Libraries for visualization\n",
    "# Библиотеки для визуализации\n",
    "\n",
    "import torchsummary\n",
    "from termcolor import cprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Libraries to hide warnings\n",
    "# Библиотека для игнорирования warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ipywidgets\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdfdae",
   "metadata": {},
   "source": [
    "# Datapaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ed1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_base_path = \"/home/sheppard/birds/train_metadata.csv\"\n",
    "test_base_path = \"/home/sheppard/birds/test.csv\"\n",
    "sample_submission_base_path = \"/home/sheppard/birds/sample_submission.csv\"\n",
    "bird_taxonomy_base_path = \"/home/sheppard/birds/eBird_Taxonomy_v2021.csv\"\n",
    "labels_base_path = \"/home/sheppard/birds/scored_birds.json\"\n",
    "train_dir = \"/home/sheppard/birds/train_audio\"\n",
    "test_dir = \"/home/sheppard/birds/test_soundscapes\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab3b7b73",
   "metadata": {},
   "source": [
    "\n",
    "Loading train metadata\n",
    "\n",
    "Загружаем тренировочные данные\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769ffb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-1.4930</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>08:00</td>\n",
       "      <td>https://www.xeno-canto.org/125458</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['houspa', 'redava', 'zebdov']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>19.8801</td>\n",
       "      <td>-155.7254</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Dan Lane</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/175522</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>16.2901</td>\n",
       "      <td>-16.0321</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>https://www.xeno-canto.org/177993</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>17.0922</td>\n",
       "      <td>54.2958</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Oscar Campbell</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:00</td>\n",
       "      <td>https://www.xeno-canto.org/205893</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>21.4581</td>\n",
       "      <td>-157.7252</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Ross Gallardy</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16:30</td>\n",
       "      <td>https://www.xeno-canto.org/207431</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                secondary_labels                     type  \\\n",
       "0       afrsil1                              []  ['call', 'flight call']   \n",
       "1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n",
       "2       afrsil1                              []         ['call', 'song']   \n",
       "3       afrsil1                              []   ['alarm call', 'call']   \n",
       "4       afrsil1                              []          ['flight call']   \n",
       "\n",
       "   latitude  longitude  scientific_name         common_name          author  \\\n",
       "0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n",
       "1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n",
       "2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n",
       "3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n",
       "4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n",
       "\n",
       "                                 url              filename  \n",
       "0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg  \n",
       "1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg  \n",
       "2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg  \n",
       "3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg  \n",
       "4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv(train_base_path)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209f146",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e04c940",
   "metadata": {},
   "source": [
    "\n",
    "Now we have to process training data so that it would be helpful for us.\n",
    "\n",
    "Теперь мы должны обработать данные обучения так, чтобы они были полезны для нас.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0009ef2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>type</th>\n",
       "      <th>rating</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>2.5</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['call']</td>\n",
       "      <td>3.5</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>3.0</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                     type  rating              filename\n",
       "0       afrsil1  ['call', 'flight call']     2.5  afrsil1/XC125458.ogg\n",
       "1       afrsil1                 ['call']     3.5  afrsil1/XC175522.ogg\n",
       "2       afrsil1         ['call', 'song']     4.0  afrsil1/XC177993.ogg\n",
       "3       afrsil1   ['alarm call', 'call']     4.0  afrsil1/XC205893.ogg\n",
       "4       afrsil1          ['flight call']     3.0  afrsil1/XC207431.ogg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imp_features = [\"primary_label\", \"type\", \"rating\", \"filename\"]\n",
    "train_df = train_df[imp_features]\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "049dfc19",
   "metadata": {},
   "source": [
    "\n",
    "In this scenario we are only taking the calls which only resemble a proper call not some specific or unique call , cause those will destroy the patterns.\n",
    "\n",
    "В этом сценарии мы принимаем только те вызовы, которые только напоминают правильный вызов, а не какой-то конкретный или уникальный вызов, потому что они разрушат шаблоны.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e4cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_call(data, call = 'call'):\n",
    "    try:\n",
    "        if re.search(data, call):\n",
    "            return \"True\"\n",
    "        else:\n",
    "            return \"False\"\n",
    "    except:\n",
    "        return \"False\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022bbaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data before call extraction : 14852\n",
      "Length of data after call extraction : 12773\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>rating</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label  rating              filename\n",
       "0       afrsil1     2.5  afrsil1/XC125458.ogg\n",
       "1       afrsil1     3.5  afrsil1/XC175522.ogg\n",
       "2       afrsil1     4.0  afrsil1/XC177993.ogg\n",
       "3       afrsil1     4.0  afrsil1/XC205893.ogg\n",
       "4       afrsil1     3.0  afrsil1/XC207431.ogg"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Length of data before call extraction : {}\".format(len(train_df)))\n",
    "train_df[\"type\"] = train_df[\"type\"].apply(extract_call)\n",
    "train_df = train_df[train_df[\"type\"] == \"True\"]\n",
    "train_df.drop(\"type\", 1, inplace = True)\n",
    "print(\"Length of data after call extraction : {}\".format(len(train_df)))\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6275fd0e",
   "metadata": {},
   "source": [
    "\n",
    "Creating a class encoding dictionary which will help us find the correct class names in future.\n",
    "\n",
    "Создание словаря кодирования классов, который поможет нам найти правильные имена классов в будущем.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078c6da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'afrsil1', 1: 'akekee', 2: 'akepa1', 3: 'akiapo', 4: 'akikik', 5: 'amewig', 6: 'aniani', 7: 'apapan', 8: 'arcter', 9: 'barpet', 10: 'bcnher', 11: 'belkin1', 12: 'bkbplo', 13: 'bknsti', 14: 'bkwpet', 15: 'blkfra', 16: 'blknod', 17: 'bongul', 18: 'brant', 19: 'brnboo', 20: 'brnnod', 21: 'brnowl', 22: 'brtcur', 23: 'bubsan', 24: 'buffle', 25: 'bulpet', 26: 'burpar', 27: 'buwtea', 28: 'cacgoo1', 29: 'calqua', 30: 'cangoo', 31: 'canvas', 32: 'caster1', 33: 'categr', 34: 'chbsan', 35: 'chemun', 36: 'chukar', 37: 'cintea', 38: 'comgal1', 39: 'commyn', 40: 'compea', 41: 'comsan', 42: 'comwax', 43: 'coopet', 44: 'crehon', 45: 'dunlin', 46: 'elepai', 47: 'ercfra', 48: 'eurwig', 49: 'fragul', 50: 'gadwal', 51: 'gamqua', 52: 'glwgul', 53: 'gnwtea', 54: 'golphe', 55: 'grbher3', 56: 'grefri', 57: 'gresca', 58: 'gryfra', 59: 'gwfgoo', 60: 'hawama', 61: 'hawcoo', 62: 'hawcre', 63: 'hawgoo', 64: 'hawhaw', 65: 'hawpet1', 66: 'hoomer', 67: 'houfin', 68: 'houspa', 69: 'hudgod', 70: 'iiwi', 71: 'incter1', 72: 'jabwar', 73: 'japqua', 74: 'kalphe', 75: 'kauama', 76: 'laugul', 77: 'layalb', 78: 'lcspet', 79: 'leasan', 80: 'leater1', 81: 'lessca', 82: 'lesyel', 83: 'lobdow', 84: 'lotjae', 85: 'madpet', 86: 'magpet1', 87: 'mallar3', 88: 'masboo', 89: 'mauala', 90: 'maupar', 91: 'merlin', 92: 'mitpar', 93: 'moudov', 94: 'norcar', 95: 'norhar2', 96: 'normoc', 97: 'norpin', 98: 'norsho', 99: 'nutman', 100: 'oahama', 101: 'omao', 102: 'osprey', 103: 'pagplo', 104: 'palila', 105: 'parjae', 106: 'pecsan', 107: 'peflov', 108: 'perfal', 109: 'pibgre', 110: 'pomjae', 111: 'puaioh', 112: 'reccar', 113: 'redava', 114: 'redjun', 115: 'redpha1', 116: 'refboo', 117: 'rempar', 118: 'rettro', 119: 'ribgul', 120: 'rinduc', 121: 'rinphe', 122: 'rocpig', 123: 'rorpar', 124: 'rudtur', 125: 'ruff', 126: 'saffin', 127: 'sander', 128: 'semplo', 129: 'sheowl', 130: 'shtsan', 131: 'skylar', 132: 'snogoo', 133: 'sooshe', 134: 'sooter1', 135: 'sopsku1', 136: 'sora', 137: 'spodov', 138: 'sposan', 139: 'towsol', 140: 'wantat1', 141: 'warwhe1', 142: 'wesmea', 143: 'wessan', 144: 'wetshe', 145: 'whfibi', 146: 'whiter', 147: 'whttro', 148: 'wiltur', 149: 'yebcar', 150: 'yefcan', 151: 'zebdov'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_dict = dict()\n",
    "\n",
    "for index, label in enumerate(train_df.primary_label.unique()):\n",
    "    class_dict[index] = label\n",
    "    train_df[\"primary_label\"].replace(label, index, inplace = True)\n",
    "print(class_dict)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10b9bfcd",
   "metadata": {},
   "source": [
    "\n",
    "Saving the object in a file so that we can use it on further cases.\n",
    "\n",
    "Сохранение объекта в файле, чтобы мы могли использовать его в дальнейшем.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d895a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json.dump(class_dict, open(\"class_dict.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2236d02",
   "metadata": {},
   "source": [
    "\n",
    "Let's check the processed training metadata.\n",
    "\n",
    "Проверим обработанные обучающие метаданные.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6abc939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>rating</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   primary_label  rating              filename\n",
       "0              0     2.5  afrsil1/XC125458.ogg\n",
       "1              0     3.5  afrsil1/XC175522.ogg\n",
       "2              0     4.0  afrsil1/XC177993.ogg\n",
       "3              0     4.0  afrsil1/XC205893.ogg\n",
       "4              0     3.0  afrsil1/XC207431.ogg"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77d3fc03",
   "metadata": {},
   "source": [
    "\n",
    "Similarly saving this for using in custom dataset and for future.\n",
    "\n",
    "Аналогично сохраняем это для использования в пользовательском наборе данных и в будущем.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac92b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df.to_csv(\"/home/sheppard/birds/training_metadata.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a6d39",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b064bcd",
   "metadata": {},
   "source": [
    "\n",
    "The first task is to fix the random seed i.e. we can replicate all the next scenarios. also setting the audio backend to lod the audio data into tensors.\n",
    "\n",
    "Первая задача - исправить случайное начальное значение, т.е. мы можем воспроизвести все следующие сценарии. также настроить аудиобэкенд для загрузки аудио в тензоры.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34589407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b9dca46",
   "metadata": {},
   "source": [
    "\n",
    "Now , it's time to build our custom dataset which will take the data directory and the processed training metadata and create trainable data.\n",
    "\n",
    "Теперь пришло время создать наш пользовательский набор данных, который возьмет каталог данных и обработанные обучающие метаданные и создаст обучаемые данные.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b04b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CLEFDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_dir,\n",
    "                 metadata_path,\n",
    "                 size = 640,\n",
    "                 transform = None):\n",
    "        super(CLEFDataset, self).__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.metadata.loc[index, \"filename\"]\n",
    "        path = os.path.join(self.data_dir, path)\n",
    "        label = self.metadata.loc[index, \"primary_label\"]\n",
    "        mono_audio = self.load_audio(path)\n",
    "        mono_audio = mono_audio.unsqueeze(dim=0)\n",
    "        return mono_audio, label\n",
    "    \n",
    "    \n",
    "    def load_audio(self, path):\n",
    "        audio, _ = torchaudio.load(path)\n",
    "        if self.transform != None:\n",
    "            for aug in self.transform:\n",
    "                audio = aug(audio)\n",
    "        return audio[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "We also need a bit of data equivalence, so that training can be more specific.\n",
    "\n",
    "Нам также нужна некоторая эквивалентность данных, чтобы обучение могло быть более конкретным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a1d078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MelSpectrogram(\n",
       "   (spectrogram): Spectrogram()\n",
       "   (mel_scale): MelScale()\n",
       " ),\n",
       " Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "augm = [\n",
    "    MelSpectrogram(n_mels = 128),\n",
    "    Resize((128, 128))\n",
    "]\n",
    "augm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Now creating the dataset.\n",
    "\n",
    "Создадим датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1313285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata_path = \"/home/sheppard/birds/training_metadata.csv\"\n",
    "dataset = CLEFDataset(train_dir, metadata_path, transform = augm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d24c9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12773"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2db095b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Shape : torch.Size([1, 128, 128]) , label : 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data, label = dataset[10]\n",
    "print(\"Audio Shape : {} , label : {}\".format(data.shape, label))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23eefe4e",
   "metadata": {},
   "source": [
    "\n",
    "The dataset is created correctly. Now we should split the dataset into training and validation sets.\n",
    "\n",
    "Train-Validation Ratio = 4:1.\n",
    "\n",
    "Датасет создан правильно. Теперь мы должны разделить набор данных на обучающий и проверочный наборы.\n",
    "\n",
    "Соотношение обучения и валидации = 4:1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a30742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = int(len(dataset) * 0.8)\n",
    "x2 = len(dataset) - x1\n",
    "train_ds, val_ds = random_split(dataset, [x1, x2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60ad54ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Dataset : 10218\n",
      "Length of Validation Dataset : 2555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Length of Training Dataset : {}\".format(len(train_ds)))\n",
    "print(\"Length of Validation Dataset : {}\".format(len(val_ds)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c7e1d56",
   "metadata": {},
   "source": [
    "\n",
    "Now it's time to create patch of data which will be a better way to train the model as it won't need too much space to load the whole data but patches of it. Note : The datasets are shuffled so that sparsity stays present.\n",
    "\n",
    "Теперь пришло время создать фрагмент данных, который будет лучшим способом обучения модели, поскольку ей не потребуется слишком много места для загрузки всех данных, кроме их фрагментов. Примечание: Наборы данных перемешиваются таким образом, чтобы сохранялась разреженность.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb15073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_dl = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "020250ed",
   "metadata": {},
   "source": [
    "\n",
    "Let's check the data chunks.\n",
    "\n",
    "Проверим блок данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd848d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 128, 128]) torch.Size([64])\n",
      "torch.Size([64, 1, 128, 128]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for patch, labels in train_dl:\n",
    "    print(patch.shape, labels.shape)\n",
    "    break\n",
    "for patch, labels in val_dl:\n",
    "    print(patch.shape, labels.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951497a",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae6fe90c",
   "metadata": {},
   "source": [
    "\n",
    "In here we have used several CNN and ANN layers just to be sure we do not leave any crucial data.\n",
    "\n",
    "Здесь мы использовали несколько слоев CNN и ANN, просто чтобы быть уверенными, что мы не теряем никаких важных данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3efaa095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convolution shape updating function\n",
    "# Функция обновления формы свертки\n",
    "\n",
    "def conv_shape(shape, kernel_size, stride, padding):\n",
    "    H, W = shape[0], shape[1]\n",
    "    H = ((H - kernel_size + 2*padding) // stride) + 1\n",
    "    W = ((W - kernel_size + 2*padding) // stride) + 1\n",
    "    return H, W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4ae5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                   in_channels,\n",
    "                   out_channels,\n",
    "                   kernel_size,\n",
    "                   stride=(1,1),\n",
    "                   padding=(0,0),\n",
    "                   momentum=0.15):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels, momentum = momentum),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class CLEFNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 in_channels = 1,\n",
    "                 H = 128,\n",
    "                 W = 128,\n",
    "                 num_downs = 3):\n",
    "        super(CLEFNetwork, self).__init__()\n",
    "        \n",
    "        self.num_C = num_classes\n",
    "        self.num_downs = num_downs\n",
    "        self.in_channels = in_channels\n",
    "        self.C = 8\n",
    "        self.H, self.W = self.calc_HW(H, W)\n",
    "        self.in_conv_block = Conv(self.in_channels, self.C, 7, (2, 2))\n",
    "        self.conv_block = nn.ModuleList(\n",
    "                [\n",
    "                    Conv(self.C * 2**i, self.C * 2**(i+1), 3, (2, 2))\n",
    "                    for i in range(self.num_downs-1)\n",
    "                ]\n",
    "        )\n",
    "        self.fc_block = nn.Sequential(\n",
    "                nn.Linear(self.H * self.W * self.C * 2**(self.num_downs - 1), 1024),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.Linear(1024, self.num_C)\n",
    "        )\n",
    "        \n",
    "    def calc_HW(self, H, W):\n",
    "        H, W = conv_shape((H, W), 7, 2, 0)\n",
    "        for num_down in range(self.num_downs - 1):\n",
    "            H, W = conv_shape((H, W), 3, 2, 0)\n",
    "        return H, W\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.in_conv_block(x)\n",
    "        for block in self.conv_block:\n",
    "            x = block(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5921047c",
   "metadata": {},
   "source": [
    "\n",
    "Now loading the class label dictionary which contains the tital number of classes.\n",
    "\n",
    "Теперь загружаем словарь меток классов, который содержит общее количество классов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c994b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_labels_path = \"./class_dict.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f209e84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class : 152\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_labels = json.load(open(class_labels_path, \"r\"))\n",
    "num_classes = len(class_labels.keys())\n",
    "print(\"Number of class : {}\".format(num_classes))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b38cc5b4",
   "metadata": {},
   "source": [
    "\n",
    "Let's try a simple forward pass with some random data on the model.\n",
    "\n",
    "Попробуем простой прямой проход с некоторыми случайными данными о модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8b7f0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 152])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = CLEFNetwork(num_classes)\n",
    "rand_data = torch.rand(5, 1, 128, 128)\n",
    "model(rand_data).shape\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "554e3fb8",
   "metadata": {},
   "source": [
    "\n",
    "Before starting the training , let's check whether all the layers are passing through the model parameters, otherwise they won't be updated with the gradients on backtracking.\n",
    "\n",
    "Перед началом обучения давайте проверим, все ли слои проходят через параметры модели, в противном случае они не будут обновлены градиентами при обратном проходе.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6074daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_conv_block.conv_block.0.weight : torch.Size([1]), requires_grad : True\n",
      "in_conv_block.conv_block.0.bias : torch.Size([1]), requires_grad : True\n",
      "in_conv_block.conv_block.1.weight : torch.Size([8, 1, 7, 7]), requires_grad : True\n",
      "in_conv_block.conv_block.1.bias : torch.Size([8]), requires_grad : True\n",
      "conv_block.0.conv_block.0.weight : torch.Size([8]), requires_grad : True\n",
      "conv_block.0.conv_block.0.bias : torch.Size([8]), requires_grad : True\n",
      "conv_block.0.conv_block.1.weight : torch.Size([16, 8, 3, 3]), requires_grad : True\n",
      "conv_block.0.conv_block.1.bias : torch.Size([16]), requires_grad : True\n",
      "conv_block.1.conv_block.0.weight : torch.Size([16]), requires_grad : True\n",
      "conv_block.1.conv_block.0.bias : torch.Size([16]), requires_grad : True\n",
      "conv_block.1.conv_block.1.weight : torch.Size([32, 16, 3, 3]), requires_grad : True\n",
      "conv_block.1.conv_block.1.bias : torch.Size([32]), requires_grad : True\n",
      "fc_block.0.weight : torch.Size([1024, 6272]), requires_grad : True\n",
      "fc_block.0.bias : torch.Size([1024]), requires_grad : True\n",
      "fc_block.1.weight : torch.Size([1024, 1024]), requires_grad : True\n",
      "fc_block.1.bias : torch.Size([1024]), requires_grad : True\n",
      "fc_block.2.weight : torch.Size([152, 1024]), requires_grad : True\n",
      "fc_block.2.bias : torch.Size([152]), requires_grad : True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "  print(f\"{name} : {param.shape}, requires_grad : {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbe7bb48",
   "metadata": {},
   "source": [
    "\n",
    "All looks fine. Let's visualize the model.\n",
    "\n",
    "Всё выглядит хорошо. Попробуем визуализировать модель.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98cfcc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1          [-1, 1, 128, 128]               2\n",
      "            Conv2d-2            [-1, 8, 61, 61]             400\n",
      "              ReLU-3            [-1, 8, 61, 61]               0\n",
      "              Conv-4            [-1, 8, 61, 61]               0\n",
      "       BatchNorm2d-5            [-1, 8, 61, 61]              16\n",
      "            Conv2d-6           [-1, 16, 30, 30]           1,168\n",
      "              ReLU-7           [-1, 16, 30, 30]               0\n",
      "              Conv-8           [-1, 16, 30, 30]               0\n",
      "       BatchNorm2d-9           [-1, 16, 30, 30]              32\n",
      "           Conv2d-10           [-1, 32, 14, 14]           4,640\n",
      "             ReLU-11           [-1, 32, 14, 14]               0\n",
      "             Conv-12           [-1, 32, 14, 14]               0\n",
      "           Linear-13                 [-1, 1024]       6,423,552\n",
      "           Linear-14                 [-1, 1024]       1,049,600\n",
      "           Linear-15                  [-1, 152]         155,800\n",
      "================================================================\n",
      "Total params: 7,635,210\n",
      "Trainable params: 7,635,210\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 1.63\n",
      "Params size (MB): 29.13\n",
      "Estimated Total Size (MB): 30.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torchsummary.summary(model, (1, 128, 128), device = \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acffbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "We need an accuracy counting function for training purpose.\n",
    "\n",
    "Нам нужна функция подсчета точности для цели обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e46772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_func(pred, true):\n",
    "    pred = torch.argmax(pred, dim = 1)\n",
    "    acc = sum(true == pred)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b13e5e",
   "metadata": {},
   "source": [
    "# Model training and saving best models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6c27142",
   "metadata": {},
   "source": [
    "\n",
    "The first task in these phase is to set the hyperparameters.\n",
    "\n",
    "Also we need to check whether any distributive device (GPU , TPU) is present or not as it may be efficient for model training.\n",
    "\n",
    "\n",
    "Первой задачей на этом этапе является установка гиперпараметров.\n",
    "\n",
    "Также нам нужно проверить, присутствует ли какое-либо распределительное устройство (GPU, TPU) или нет, поскольку это может быть эффективно для обучения модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09875280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 10\n",
    "optim = Adam(model.parameters(), lr = 1e-4)\n",
    "criterion = CrossEntropyLoss()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd8e6c39",
   "metadata": {},
   "source": [
    "Training:\n",
    "\n",
    "Now it is the most important moment of the whole task.\n",
    "\n",
    "The training loop will take the best model on the accuracy and loss metrics.\n",
    "\n",
    "Обучение:\n",
    "\n",
    "Теперь это самый важный момент во всей задаче.\n",
    "\n",
    "Цикл обучения выберет наилучшую модель по показателям точности и потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e59d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training...\n",
      "Model Loaded on GPU...\n",
      "Epoch 1 :\n",
      "      [Step 10] Loss : 4.818160\n",
      "      [Step 20] Loss : 4.583258\n",
      "      [Step 30] Loss : 4.389035\n",
      "      [Step 40] Loss : 3.980569\n",
      "      [Step 50] Loss : 4.243809\n",
      "      [Step 60] Loss : 4.216793\n",
      "      [Step 70] Loss : 4.121331\n",
      "      [Step 80] Loss : 4.277386\n",
      "      [Step 90] Loss : 4.263049\n",
      "      [Step 100] Loss : 3.929524\n",
      "      [Step 110] Loss : 4.155272\n",
      "      [Step 120] Loss : 4.431566\n",
      "      [Step 130] Loss : 4.255744\n",
      "      [Step 140] Loss : 4.092552\n",
      "      [Step 150] Loss : 4.111627\n",
      "      [Step 160] Loss : 4.179520\n",
      "   Train Loss : 690.738345 | Train accuracy : 0.070464\n",
      "   Validation Loss : 163.600589 | Validation Accuracy : 0.094716\n",
      "Loss Updation : Positive\n",
      "Accuracy Updation : Positive\n",
      "   Execution Time : 944.470 seconds\n",
      "\n",
      "Epoch 2 :\n",
      "      [Step 10] Loss : 3.883369\n",
      "      [Step 20] Loss : 3.687297\n",
      "      [Step 30] Loss : 3.999624\n",
      "      [Step 40] Loss : 3.846545\n",
      "      [Step 50] Loss : 3.689872\n",
      "      [Step 60] Loss : 3.755167\n",
      "      [Step 70] Loss : 3.850843\n",
      "      [Step 80] Loss : 4.049751\n",
      "      [Step 90] Loss : 3.868900\n",
      "      [Step 100] Loss : 3.561264\n",
      "      [Step 110] Loss : 3.698797\n",
      "      [Step 120] Loss : 3.655242\n",
      "      [Step 130] Loss : 3.564814\n",
      "      [Step 140] Loss : 3.564631\n",
      "      [Step 150] Loss : 3.503134\n",
      "      [Step 160] Loss : 3.721727\n",
      "   Train Loss : 601.355369 | Train accuracy : 0.155118\n",
      "   Validation Loss : 153.557867 | Validation Accuracy : 0.144031\n",
      "Loss Updation : Positive\n",
      "Accuracy Updation : Positive\n",
      "   Execution Time : 945.572 seconds\n",
      "\n",
      "Epoch 3 :\n",
      "      [Step 10] Loss : 3.416924\n",
      "      [Step 20] Loss : 3.577319\n",
      "      [Step 30] Loss : 3.658917\n",
      "      [Step 40] Loss : 3.253761\n",
      "      [Step 50] Loss : 3.135112\n",
      "      [Step 60] Loss : 3.398364\n",
      "      [Step 70] Loss : 3.801661\n",
      "      [Step 80] Loss : 3.313427\n",
      "      [Step 90] Loss : 3.555573\n",
      "      [Step 100] Loss : 3.359082\n",
      "      [Step 110] Loss : 3.293234\n",
      "      [Step 120] Loss : 3.283673\n",
      "      [Step 130] Loss : 3.428104\n",
      "      [Step 140] Loss : 3.296378\n",
      "      [Step 150] Loss : 3.279011\n",
      "      [Step 160] Loss : 3.370317\n",
      "   Train Loss : 549.351499 | Train accuracy : 0.217949\n",
      "   Validation Loss : 148.607896 | Validation Accuracy : 0.157730\n",
      "Loss Updation : Positive\n",
      "Accuracy Updation : Positive\n",
      "   Execution Time : 932.540 seconds\n",
      "\n",
      "Epoch 4 :\n",
      "      [Step 10] Loss : 3.084570\n",
      "      [Step 20] Loss : 3.242570\n",
      "      [Step 30] Loss : 3.429140\n",
      "      [Step 40] Loss : 3.413117\n",
      "      [Step 50] Loss : 3.067893\n",
      "      [Step 60] Loss : 3.074013\n",
      "      [Step 70] Loss : 2.865467\n",
      "      [Step 80] Loss : 3.233529\n",
      "      [Step 90] Loss : 3.430365\n",
      "      [Step 100] Loss : 2.885875\n",
      "      [Step 110] Loss : 3.401029\n",
      "      [Step 120] Loss : 3.375008\n",
      "      [Step 130] Loss : 3.161229\n",
      "      [Step 140] Loss : 3.350174\n",
      "      [Step 150] Loss : 3.262774\n",
      "      [Step 160] Loss : 3.061666\n",
      "   Train Loss : 507.725574 | Train accuracy : 0.268252\n",
      "   Validation Loss : 148.055667 | Validation Accuracy : 0.172994\n",
      "Loss Updation : Positive\n",
      "Accuracy Updation : Positive\n",
      "   Execution Time : 984.166 seconds\n",
      "\n",
      "Epoch 5 :\n",
      "      [Step 10] Loss : 3.230816\n",
      "      [Step 20] Loss : 3.120880\n",
      "      [Step 30] Loss : 3.013654\n",
      "      [Step 40] Loss : 2.700721\n",
      "      [Step 50] Loss : 2.657244\n",
      "      [Step 60] Loss : 3.048238\n",
      "      [Step 70] Loss : 3.183911\n",
      "      [Step 80] Loss : 3.031121\n",
      "      [Step 90] Loss : 2.507579\n",
      "      [Step 100] Loss : 3.060454\n",
      "      [Step 110] Loss : 2.750046\n",
      "      [Step 120] Loss : 2.702097\n",
      "      [Step 130] Loss : 2.952696\n",
      "      [Step 140] Loss : 3.194645\n",
      "      [Step 150] Loss : 3.097422\n",
      "      [Step 160] Loss : 2.670846\n",
      "   Train Loss : 472.828997 | Train accuracy : 0.320904\n",
      "   Validation Loss : 147.302636 | Validation Accuracy : 0.166341\n",
      "Loss Updation : Positive\n",
      "   Execution Time : 1008.915 seconds\n",
      "\n",
      "Epoch 6 :\n",
      "      [Step 10] Loss : 3.177475\n",
      "      [Step 20] Loss : 2.489420\n",
      "      [Step 30] Loss : 2.640169\n",
      "      [Step 40] Loss : 2.330661\n",
      "      [Step 50] Loss : 2.918089\n",
      "      [Step 60] Loss : 2.885355\n",
      "      [Step 70] Loss : 2.704659\n",
      "      [Step 80] Loss : 2.620029\n",
      "      [Step 90] Loss : 2.716357\n",
      "      [Step 100] Loss : 3.037685\n",
      "      [Step 110] Loss : 2.464430\n",
      "      [Step 120] Loss : 2.989047\n",
      "      [Step 130] Loss : 2.605974\n",
      "      [Step 140] Loss : 2.648543\n",
      "      [Step 150] Loss : 2.841059\n",
      "      [Step 160] Loss : 3.336342\n",
      "   Train Loss : 441.002851 | Train accuracy : 0.359855\n",
      "   Validation Loss : 149.258090 | Validation Accuracy : 0.202740\n",
      "Accuracy Updation : Positive\n",
      "   Execution Time : 1018.500 seconds\n",
      "\n",
      "Epoch 7 :\n",
      "      [Step 10] Loss : 2.608664\n",
      "      [Step 20] Loss : 2.037772\n",
      "      [Step 30] Loss : 3.000232\n",
      "      [Step 40] Loss : 2.571860\n",
      "      [Step 50] Loss : 3.029349\n",
      "      [Step 60] Loss : 2.507823\n",
      "      [Step 70] Loss : 3.077249\n",
      "      [Step 80] Loss : 2.800809\n",
      "      [Step 90] Loss : 3.032967\n",
      "      [Step 100] Loss : 2.473070\n",
      "      [Step 110] Loss : 2.244561\n",
      "      [Step 120] Loss : 2.274682\n",
      "      [Step 130] Loss : 2.136465\n",
      "      [Step 140] Loss : 2.309038\n",
      "      [Step 150] Loss : 3.189076\n",
      "      [Step 160] Loss : 1.910559\n",
      "   Train Loss : 408.870376 | Train accuracy : 0.404482\n",
      "   Validation Loss : 151.331292 | Validation Accuracy : 0.178082\n",
      "Model Updation : Negative\n",
      "\n",
      "   Execution Time : 969.277 seconds\n",
      "\n",
      "Epoch 8 :\n",
      "      [Step 10] Loss : 2.123992\n",
      "      [Step 20] Loss : 2.493082\n",
      "      [Step 30] Loss : 2.415715\n",
      "      [Step 40] Loss : 2.223982\n",
      "      [Step 50] Loss : 2.109831\n",
      "      [Step 60] Loss : 2.468638\n",
      "      [Step 70] Loss : 2.712218\n",
      "      [Step 80] Loss : 2.680320\n",
      "      [Step 90] Loss : 2.409901\n",
      "      [Step 100] Loss : 1.916175\n",
      "      [Step 110] Loss : 2.717845\n",
      "      [Step 120] Loss : 2.249680\n",
      "      [Step 130] Loss : 2.521242\n",
      "      [Step 140] Loss : 2.116314\n",
      "      [Step 150] Loss : 2.614486\n",
      "      [Step 160] Loss : 2.234549\n",
      "   Train Loss : 387.084229 | Train accuracy : 0.428068\n",
      "   Validation Loss : 158.346307 | Validation Accuracy : 0.186301\n",
      "Model Updation : Negative\n",
      "\n",
      "   Execution Time : 962.180 seconds\n",
      "\n",
      "Epoch 9 :\n",
      "      [Step 10] Loss : 2.378162\n",
      "      [Step 20] Loss : 2.352539\n",
      "      [Step 30] Loss : 2.452515\n",
      "      [Step 40] Loss : 2.462016\n",
      "      [Step 50] Loss : 2.355135\n",
      "      [Step 60] Loss : 2.359342\n",
      "      [Step 70] Loss : 2.293557\n",
      "      [Step 80] Loss : 2.108824\n",
      "      [Step 90] Loss : 2.256771\n",
      "      [Step 100] Loss : 2.668775\n",
      "      [Step 110] Loss : 1.917181\n",
      "      [Step 120] Loss : 1.846504\n",
      "      [Step 130] Loss : 2.009114\n",
      "      [Step 140] Loss : 1.947397\n",
      "      [Step 150] Loss : 2.476791\n",
      "      [Step 160] Loss : 1.883116\n",
      "   Train Loss : 356.351651 | Train accuracy : 0.476708\n",
      "   Validation Loss : 160.101684 | Validation Accuracy : 0.196869\n",
      "Model Updation : Negative\n",
      "\n",
      "   Execution Time : 953.238 seconds\n",
      "\n",
      "Epoch 10 :\n",
      "      [Step 10] Loss : 1.829714\n",
      "      [Step 20] Loss : 2.174692\n",
      "      [Step 30] Loss : 1.846033\n",
      "      [Step 40] Loss : 1.988148\n",
      "      [Step 50] Loss : 2.170020\n",
      "      [Step 60] Loss : 1.870994\n",
      "      [Step 70] Loss : 1.952243\n",
      "      [Step 80] Loss : 1.961148\n",
      "      [Step 90] Loss : 2.190104\n",
      "      [Step 100] Loss : 1.922880\n",
      "      [Step 110] Loss : 2.324351\n",
      "      [Step 120] Loss : 1.886012\n",
      "      [Step 130] Loss : 1.962724\n",
      "      [Step 140] Loss : 2.046631\n",
      "      [Step 150] Loss : 2.028933\n",
      "      [Step 160] Loss : 2.128719\n",
      "   Train Loss : 332.328086 | Train accuracy : 0.507634\n",
      "   Validation Loss : 161.922352 | Validation Accuracy : 0.200783\n",
      "Model Updation : Negative\n",
      "\n",
      "   Execution Time : 934.324 seconds\n",
      "\n",
      "Training finished...\n",
      "Exceution Time : 9654.960 seconds\n"
     ]
    }
   ],
   "source": [
    "train_init = time.time()\n",
    "cprint(\"Started training...\", \"blue\")\n",
    "best_loss = np.inf\n",
    "best_acc = 0.0\n",
    "if device == \"cuda:0\":\n",
    "    print(\"Model Loaded on GPU...\")\n",
    "    model = model.cuda()\n",
    "update = 0\n",
    "TL, VL, TA, VA = [], [], [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1} :\")\n",
    "    epoch_init = time.time()\n",
    "    train_loss = val_loss = 0.0\n",
    "    train_acc = val_acc = 0\n",
    "    tot_val_data_point = 0\n",
    "    model.train()\n",
    "    for train_index, (patch, labels) in enumerate(train_dl):\n",
    "        optim.zero_grad()\n",
    "        if device == \"cuda:0\":\n",
    "            dev_patch = patch.cuda()\n",
    "            dev_labels = labels.cuda()\n",
    "        else:\n",
    "            dev_patch = patch\n",
    "            dev_labels = labels\n",
    "        output = model(dev_patch)\n",
    "        acc = accuracy_func(output, dev_labels)\n",
    "        train_acc += acc\n",
    "        loss = criterion(output, dev_labels)\n",
    "        train_loss += loss.item()\n",
    "        TL.append(loss.item())\n",
    "        TA.append(acc / dev_patch.shape[0])\n",
    "        if train_index % 10 == 9:\n",
    "            print(f\"      [Step {train_index + 1}] Loss : {'%.6f'%loss.item()}\")\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_index, (patch, labels) in enumerate(val_dl):\n",
    "            if device == \"cuda:0\":\n",
    "                dev_patch = patch.cuda()\n",
    "                dev_labels = labels.cuda()\n",
    "            else:\n",
    "              dev_patch = patch\n",
    "              dev_labels = labels\n",
    "            output = model(dev_patch)\n",
    "            acc = accuracy_func(output, dev_labels)\n",
    "            val_acc += acc\n",
    "            loss = criterion(output, dev_labels)\n",
    "            val_loss += loss.item()\n",
    "            VL.append(loss.item())\n",
    "            VA.append(acc / dev_patch.shape[0])\n",
    "    TRAIN_ACC = train_acc / len(train_ds)\n",
    "    VAL_ACC = val_acc / len(val_ds)\n",
    "    print(f\"   Train Loss : {'%.6f'%train_loss} | Train accuracy : {'%.6f'%TRAIN_ACC}\")\n",
    "    print(f\"   Validation Loss : {'%.6f'%val_loss} | Validation Accuracy : {'%.6f'%VAL_ACC}\")\n",
    "    updation_flag = False\n",
    "    if val_loss < best_loss:\n",
    "      update = 0\n",
    "      updation_flag = True\n",
    "      best_loss = val_loss\n",
    "      cprint(\"Loss Updation : Positive\", \"green\")\n",
    "      torch.save({\n",
    "          \"model\" : model.state_dict(),\n",
    "          \"optim\" : optim.state_dict(),\n",
    "          \"epoch\" : epoch + 1\n",
    "      }, \"best_loss_model.pt\")\n",
    "    if VAL_ACC > best_acc:\n",
    "      update = 0\n",
    "      updation_flag = True\n",
    "      best_acc = VAL_ACC\n",
    "      cprint(\"Accuracy Updation : Positive\", \"green\")\n",
    "      torch.save({\n",
    "          \"model\" : model.state_dict(),\n",
    "          \"optim\" : optim.state_dict(),\n",
    "          \"epoch\" : epoch + 1\n",
    "      }, \"best_accuracy_model.pt\")\n",
    "    if updation_flag == False:\n",
    "        cprint(\"Model Updation : Negative\\n\", \"red\")\n",
    "        update += 1\n",
    "    print(f\"   Execution Time : {'%.3f'%(time.time() - epoch_init)} seconds\\n\")\n",
    "    if update >= 5:\n",
    "      cprint(\"Model Stopped due to continuous model learning degradation\\n\", \"red\")\n",
    "      break\n",
    "cprint(\"Training finished...\", \"blue\")\n",
    "cprint(f\"Exceution Time : {'%.3f'%(time.time() - train_init)} seconds\", \"blue\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
